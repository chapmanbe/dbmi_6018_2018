{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Gensim Phrases Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from gensim.models import phrases\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Some Text from the MIMIC2 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter MySQL passwd for jovyan········\n"
     ]
    }
   ],
   "source": [
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>28766.0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2644-1-17**] 10:53 AM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>28766.0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2644-1-17**] 10:53 AM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>28766.0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2644-1-17**] 10:43 AM\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>28766.0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2644-1-17**] 6:37 AM\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>28766.0</td>\n",
       "      <td>\\n\\n\\n     DATE: [**2644-1-19**] 12:09 PM\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id                                               text\n",
       "0          56  28766.0  \\n\\n\\n     DATE: [**2644-1-17**] 10:53 AM\\n   ...\n",
       "1          56  28766.0  \\n\\n\\n     DATE: [**2644-1-17**] 10:53 AM\\n   ...\n",
       "2          56  28766.0  \\n\\n\\n     DATE: [**2644-1-17**] 10:43 AM\\n   ...\n",
       "3          56  28766.0  \\n\\n\\n     DATE: [**2644-1-17**] 6:37 AM\\n    ...\n",
       "4          56  28766.0  \\n\\n\\n     DATE: [**2644-1-19**] 12:09 PM\\n   ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rad_data = \\\n",
    "pd.read_sql(\"\"\"SELECT noteevents.subject_id, \n",
    "                      noteevents.hadm_id,\n",
    "                      noteevents.text \n",
    "               FROM noteevents\n",
    "               WHERE noteevents.category = 'RADIOLOGY_REPORT' LIMIT 10000\"\"\",conn)\n",
    "rad_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to get all the reports into a single string\n",
    "#### This is a great application for list comprehension\n",
    "\n",
    "* Remember the ``join`` method of a string ``a`` joins a list of string separated by the value of ``a``. For example \"\\n\".join([\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n2\\n3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join([\"1\",\"2\",\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_string = \" \".join([row[\"text\"] for _,row in rad_data.iterrows()])\n",
    "blob = TextBlob(big_string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to splitting the text into words (and tokens), the TextBlob object also splits the text into sentences uses standard English rules. There will be lots of mistakes.\n",
    "\n",
    "``blob.sentences`` will be a list of sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = blob.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence objects have word list attributes, token, word_counts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cmpkey',\n",
       " '_compare',\n",
       " '_strkey',\n",
       " 'analyzer',\n",
       " 'classifier',\n",
       " 'classify',\n",
       " 'correct',\n",
       " 'detect_language',\n",
       " 'dict',\n",
       " 'end',\n",
       " 'end_index',\n",
       " 'ends_with',\n",
       " 'endswith',\n",
       " 'find',\n",
       " 'format',\n",
       " 'index',\n",
       " 'join',\n",
       " 'lower',\n",
       " 'ngrams',\n",
       " 'noun_phrases',\n",
       " 'np_counts',\n",
       " 'np_extractor',\n",
       " 'parse',\n",
       " 'parser',\n",
       " 'polarity',\n",
       " 'pos_tagger',\n",
       " 'pos_tags',\n",
       " 'raw',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'sentiment',\n",
       " 'sentiment_assessments',\n",
       " 'split',\n",
       " 'start',\n",
       " 'start_index',\n",
       " 'starts_with',\n",
       " 'startswith',\n",
       " 'string',\n",
       " 'strip',\n",
       " 'stripped',\n",
       " 'subjectivity',\n",
       " 'tags',\n",
       " 'title',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'tokens',\n",
       " 'translate',\n",
       " 'translator',\n",
       " 'upper',\n",
       " 'word_counts',\n",
       " 'words']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sentences[0]\n",
    "dir(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase detection is done at the sentence level\n",
    "``phrases.Phrases`` needs a list of lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = [s.words for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build our phrase detectors recursively\n",
    "* We first detect two-word phrases\n",
    "* We then pass the output of the two-word phrase detector to detect three-word phrases, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_generator = phrases.Phrases(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "trigram_generator =phrases.Phrases(bigram_generator[sentences2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Phrases`` takes keyword arguments\n",
    "\n",
    "#### The one we might be most interessted in is\n",
    "\n",
    "* ``min_count`` with default of 5: The minimum number of observations in this corpus to be condidered a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Phrases in module gensim.models.phrases:\n",
      "\n",
      "class Phrases(SentenceAnalyzer, PhrasesTransformation)\n",
      " |  Detect phrases based on collocation counts.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Phrases\n",
      " |      SentenceAnalyzer\n",
      " |      PhrasesTransformation\n",
      " |      gensim.interfaces.TransformationABC\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, sentence)\n",
      " |      Convert the input tokens `sentence` into tokens where detected bigrams are joined by a selected delimiter.\n",
      " |      \n",
      " |      If `sentence` is an entire corpus (iterable of sentences rather than a single\n",
      " |      sentence), return an iterable that converts each of the corpus' sentences\n",
      " |      into phrases on the fly, one after another.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : {list of str, iterable of list of str}\n",
      " |          Sentence or text corpus.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      {list of str, :class:`gensim.interfaces.TransformedCorpus`}\n",
      " |          `sentence` with detected phrase bigrams merged together, or a streamed corpus of such sentences\n",
      " |          if the input was a corpus.\n",
      " |      \n",
      " |      Examples\n",
      " |      ----------\n",
      " |      >>> from gensim.test.utils import datapath\n",
      " |      >>> from gensim.models.word2vec import Text8Corpus\n",
      " |      >>> from gensim.models.phrases import Phrases, Phraser\n",
      " |      >>>\n",
      " |      >>> #Create corpus\n",
      " |      >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |      >>>\n",
      " |      >>> #Train the detector with:\n",
      " |      >>> phrases = Phrases(sentences, min_count=1, threshold=1)\n",
      " |      >>> #Input is a list of unicode strings:\n",
      " |      >>> sent = [u'trees', u'graph', u'minors']\n",
      " |      >>> #Both of these tokens appear in corpus at least twice, and phrase score is higher, than treshold = 1:\n",
      " |      >>> print(phrases[sent])\n",
      " |      [u'trees_graph', u'minors']\n",
      " |      >>>\n",
      " |      >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |      >>> phrases = Phrases(sentences, min_count=1, threshold=1)\n",
      " |      >>> phraser = Phraser(phrases)  # for speedup\n",
      " |      >>>\n",
      " |      >>> sent = [[u'trees', u'graph', u'minors'],[u'graph', u'minors']]\n",
      " |      >>> for phrase in phraser[sent]:\n",
      " |      ...     pass\n",
      " |  \n",
      " |  __init__(self, sentences=None, min_count=5, threshold=10.0, max_vocab_size=40000000, delimiter=b'_', progress_per=10000, scoring='default', common_terms=frozenset())\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          The `sentences` iterable can be simply a list, but for larger corpora, consider a generator that streams\n",
      " |          the sentences directly from disk/network, See :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      " |          :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`\n",
      " |          for such examples.\n",
      " |      min_count : float, optional\n",
      " |          Ignore all words and bigrams with total collected count lower than this value.\n",
      " |      threshold : float, optional\n",
      " |          Represent a score threshold for forming the phrases (higher means fewer phrases).\n",
      " |          A phrase of words `a` followed by `b` is accepted if the score of the phrase is greater than threshold.\n",
      " |          Hardly depends on concrete socring-function, see the `scoring` parameter.\n",
      " |      max_vocab_size : int, optional\n",
      " |          Maximum size (number of tokens) of the vocabulary. Used to control pruning of less common words,\n",
      " |          to keep memory under control. The default of 40M needs about 3.6GB of RAM. Increase/decrease\n",
      " |          `max_vocab_size` depending on how much available memory you have.\n",
      " |      delimiter : str, optional\n",
      " |          Glue character used to join collocation tokens, should be a byte string (e.g. b'_').\n",
      " |      scoring : {'default', 'npmi', function}, optional\n",
      " |          Specify how potential phrases are scored. `scoring` can be set with either a string that refers to a\n",
      " |          built-in scoring function, or with a function with the expected parameter names.\n",
      " |          Two built-in scoring functions are available by setting `scoring` to a string:\n",
      " |      \n",
      " |          #. \"default\" - :func:`~gensim.models.phrases.original_scorer`.\n",
      " |          #. \"npmi\" - :func:`~gensim.models.phrases.npmi_scorer`.\n",
      " |      common_terms : set of str, optional\n",
      " |          List of \"stop words\" that won't affect frequency count of expressions containing them.\n",
      " |          Allow to detect expressions like \"bank_of_america\" or \"eye_of_the_beholder\".\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      'npmi' is more robust when dealing with common words that form part of common bigrams, and\n",
      " |      ranges from -1 to 1, but is slower to calculate than the default. The default is the PMI-like scoring\n",
      " |      as described by `Mikolov, et. al: \"Distributed Representations of Words and Phrases and their Compositionality\"\n",
      " |      <https://arxiv.org/abs/1310.4546>`_.\n",
      " |      \n",
      " |      To use a custom scoring function, pass in a function with the following signature:\n",
      " |      \n",
      " |      * worda_count - number of corpus occurrences in `sentences` of the first token in the bigram being scored\n",
      " |      * wordb_count - number of corpus occurrences in `sentences` of the second token in the bigram being scored\n",
      " |      * bigram_count - number of occurrences in `sentences` of the whole bigram\n",
      " |      * len_vocab - the number of unique tokens in `sentences`\n",
      " |      * min_count - the `min_count` setting of the Phrases class\n",
      " |      * corpus_word_count - the total number of tokens (non-unique) in `sentences`\n",
      " |      \n",
      " |      The scoring function **must accept all these parameters**, even if it doesn't use them in its scoring.\n",
      " |      The scoring function **must be pickleable**.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get short string representation of this phrase detector.\n",
      " |  \n",
      " |  add_vocab(self, sentences)\n",
      " |      Update model with new `sentences`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      >>> from gensim.test.utils import datapath\n",
      " |      >>> from gensim.models.word2vec import Text8Corpus\n",
      " |      >>> from gensim.models.phrases import Phrases\n",
      " |      >>> #Create corpus and use it for phrase detector\n",
      " |      >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |      >>> phrases = Phrases(sentences)  # train model\n",
      " |      >>> assert len(phrases.vocab) == 37\n",
      " |      >>>\n",
      " |      >>> more_sentences = [\n",
      " |      ...    [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there'],\n",
      " |      ...    [u'machine', u'learning', u'can', u'be', u'new', u'york' , u'sometimes']\n",
      " |      ... ]\n",
      " |      >>>\n",
      " |      >>> phrases.add_vocab(more_sentences)  # add new sentences to model\n",
      " |      >>> assert len(phrases.vocab) == 60\n",
      " |  \n",
      " |  export_phrases(self, sentences, out_delimiter=b' ', as_tuples=False)\n",
      " |      Get all phrases that appear in 'sentences' that pass the bigram threshold.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Text corpus.\n",
      " |      out_delimiter : str, optional\n",
      " |          Delimiter used to \"glue\" together words that form a bigram phrase.\n",
      " |      as_tuples : bool, optional\n",
      " |          Yield `(tuple(words), score)` instead of `(out_delimiter.join(words), score)`?\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      ((str, str), float) **or** (str, float)\n",
      " |          Phrases detected in `sentences`. Return type depends on the `as_tuples` parameter.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      >>> from gensim.test.utils import datapath\n",
      " |      >>> from gensim.models.word2vec import Text8Corpus\n",
      " |      >>> from gensim.models.phrases import Phrases\n",
      " |      >>>\n",
      " |      >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |      >>> phrases = Phrases(sentences, min_count=1, threshold=0.1)\n",
      " |      >>>\n",
      " |      >>> for phrase, score in phrases.export_phrases(sentences):\n",
      " |      ...     pass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved Phrases class.\n",
      " |      Handles backwards compatibility from older Phrases versions which did not support pluggable scoring functions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : object\n",
      " |          Sequence of arguments, see :class:`~gensim.utils.SaveLoad.load` for more information.\n",
      " |      kwargs : object\n",
      " |          Sequence of arguments, see :class:`~gensim.utils.SaveLoad.load` for more information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  learn_vocab(sentences, max_vocab_size, delimiter=b'_', progress_per=10000, common_terms=frozenset())\n",
      " |      Collect unigram/bigram counts from the `sentences` iterable.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list, but for larger corpora, consider a generator that streams\n",
      " |          the sentences directly from disk/network, See :class:`~gensim.models.word2vec.BrownCorpus`,\n",
      " |          :class:`~gensim.models.word2vec.Text8Corpus` or :class:`~gensim.models.word2vec.LineSentence`\n",
      " |          for such examples.\n",
      " |      max_vocab_size : int\n",
      " |          Maximum size (number of tokens) of the vocabulary. Used to control pruning of less common words,\n",
      " |          to keep memory under control. The default of 40M needs about 3.6GB of RAM. Increase/decrease\n",
      " |          `max_vocab_size` depending on how much available memory you have.\n",
      " |      delimiter : str, optional\n",
      " |          Glue character used to join collocation tokens, should be a byte string (e.g. b'_').\n",
      " |      progress_per : int\n",
      " |          Write logs every `progress_per` sentence.\n",
      " |      common_terms : set of str, optional\n",
      " |          List of \"stop words\" that won't affect frequency count of expressions containing them.\n",
      " |          Allow to detect expressions like \"bank_of_america\" or \"eye_of_the_beholder\".\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      (int, dict of (str, int), int)\n",
      " |          Number of pruned words, counters for each word/bi-gram and total number of words.\n",
      " |      \n",
      " |      Example\n",
      " |      ----------\n",
      " |      >>> from gensim.test.utils import datapath\n",
      " |      >>> from gensim.models.word2vec import Text8Corpus\n",
      " |      >>> from gensim.models.phrases import Phrases\n",
      " |      >>>\n",
      " |      >>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
      " |      >>> pruned_words, counters, total_words = Phrases.learn_vocab(sentences, 100)\n",
      " |      >>> (pruned_words, total_words)\n",
      " |      (1, 29)\n",
      " |      >>> counters['computer']\n",
      " |      2\n",
      " |      >>> counters['response_time']\n",
      " |      1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SentenceAnalyzer:\n",
      " |  \n",
      " |  analyze_sentence(self, sentence, threshold, common_terms, scorer)\n",
      " |      Analyze a sentence, detecting any bigrams that should be concatenated.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentence : iterable of str\n",
      " |          Token sequence representing the sentence to be analyzed.\n",
      " |      threshold : float\n",
      " |          The minimum score for a bigram to be taken into account.\n",
      " |      common_terms : list of object\n",
      " |          List of common terms, they have special treatment.\n",
      " |      scorer : function\n",
      " |          Scorer function, as given to :class:`~gensim.models.phrases.Phrases`.\n",
      " |          See :func:`~gensim.models.phrases.npmi_scorer` and :func:`~gensim.models.phrases.original_scorer`.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      (str, score)\n",
      " |          If bi-gram detected, a tuple where the first element is a detect bigram, second its score.\n",
      " |          Otherwise, the first tuple element is a single word and second is None.\n",
      " |  \n",
      " |  score_item(self, worda, wordb, components, scorer)\n",
      " |      Get bi-gram score statistics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      worda : str\n",
      " |          First word of bi-gram.\n",
      " |      wordb : str\n",
      " |          Second word of bi-gram.\n",
      " |      components : generator\n",
      " |          Contain all phrases.\n",
      " |      scorer : function\n",
      " |          Scorer function, as given to :class:`~gensim.models.phrases.Phrases`.\n",
      " |          See :func:`~gensim.models.phrases.npmi_scorer` and :func:`~gensim.models.phrases.original_scorer`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Score for given bi-gram, if bi-gram not presented in dictionary - return -1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SentenceAnalyzer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      " |      Save the object to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |      \n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(phrases.Phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Report Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rd = re.compile(r\"\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date : [ **dddd-dd-dd** ] dd : dd am_perc_g/g-j tube_plmt_clip # [ **clip number ( radiology ) ddddd** ] reason : perm_gi_access through_pej_tube contrast : conray_amt : dd ********************************* cpt_codes ******************************** * ddddd perc_plcmt_gastromy_tube ddddd plct_gj_tube * * -dd distinct_procedural_service ddddd perc_plcmt_entroclysis_tube * * ddddd iv_conscioutious sedation_pro * **************************************************************************** ______________________________________________________________________________ underlying_medical_condition : dd year_old_man with hypoxia and b/l pna trach_'d and needing more perm feeding access . please_place_pej_not peg_tube in this patient . reason for this_examination : perm_gi_access through_pej_tube ______________________________________________________________________________ final_report_indication : dd-year-old man with hypoxia and bilateral pneumonia , status_post tracheotomy and respiratory_failure . request to place a gastrojejunostomy_tube . radiologist : dr. [ **first name ( stitle ) dddd** ] [ **name ( stitle ) ** ] d'othee , attending_radiologist , performed the entire_procedure . medications : lidocaine d % for local_anesthesia . for conscious_sedation , a total of dd microg of fentanyl_were given_intravenously in small divided_doses_under_continuous hemodynamic_monitoring by nursing_staff . a total of dd ml of conray dd % were given the g-i tract for contrast . technique/findings : the procedure was explained to the patient_'s_family and informed_consent was obtained . the patient was placed_supine_on the angiographic_table and his abdomen was sterily prepped and draped . fluoroscopy guidance was used to determine an appropriate entry_site after the stomach had_been inflated with air by the previously_placed nasogastric_tube . after_local_anesthesia , an dd gauge_needle was advanced_under_fluoroscopic guidance through the abdominal_wall into the stomach and its position was confirmed_by aspiration of air through the needle as_well_as injection of contrast . a t-fastener was deployed in the stomach and afixed to the skin . a similar maneuver was repeated twice for placement of two more t-fasteners . then , the needle was again advanced_under_fluoroscopic guidance . its position was confirmed in the stomach , followed_by advancement of an d.ddd [ **last name ( un ) dddd** ] wire through the needle into the stomach . the needle was removed and exchanged for a d french , dd cm-long sheath . the position of the sheath was confirmed under_fluoroscopic_guidance by contrast injection through the side arm of the sheath . using the combination of an d.ddd glidewire and d fr mpa-d catheter , access was gained_through the pylorus and duodenum , until the proximal_jejunum was reached . the guidewire was removed and exchanged for an d.ddd amplatz_super-stiff wire , the tip of which was left in the proximal_jejunum . the catheter and sheath were_removed and exchanged and the tract was serially_dilated_up to dd french . a dd french peelaway_sheath was advanced_over the ( over ) date : [ **dddd-dd-dd** ] dd : dd am_perc_g/g-j tube_plmt_clip # [ **clip number ( radiology ) ddddd** ] reason : perm_gi_access through_pej_tube contrast : conray_amt : dd ______________________________________________________________________________ final_report ( cont ) wire until_its_tip reached the duodenum . a dd.d french [ **doctor last_name ddd** ] gastrojejunostomy ( g-j ) tube was advanced_over the wire through the sheath until_its_tip reached the proximal_jejunum , with simultaneous peeling_away of the sheath . the locking_loop_mechanism of the tube was formed after removal of the wire and contrast was injected_through the tubes to confirm appropriate tube position . an ap x-ray of the abdomen was obtained . this showed appropriate_positioning of the tube , with the locking_loop_mechanism at the level of the pylorus and the tip at the level of the angle of treitz/proximal jejunal_loop . no extravasation of contrast seen . contrast is seen in the greater_tuberosity of the stomach as_well_as the duodenum and proximal jejunal_loops . the tube was then_sutured to the skin with an d prolene_suture as_well_as affixed with a flexitrack . a sterile_dressing was applied . complications : no immediate_complication_occurred . conclusion : successful_placement of a percutaneous_gastrojejunostomy_tube , the tip of which is in the proximal_jejunum . please wait until tomorrow morning to remove the nasogastric_tube and start using the gastrojejunostomy_tube , pending stable condition .\n"
     ]
    }
   ],
   "source": [
    "num_reports = rad_data.shape[0]\n",
    "while True:\n",
    "    try:\n",
    "        i = int(input(\"Enter a number between 0 and %d. otherwise to quit\"%num_reports))\n",
    "        clear_output()\n",
    "\n",
    "        if i < 0 or i >=num_reports:\n",
    "            break\n",
    "        txt = TextBlob(rd.sub(\"\"\"d\"\"\", rad_data.iloc[i]['text'].strip().lower()))\n",
    "        print(\" \".join(trigram_generator[bigram_generator[txt.tokens]]))\n",
    "        \n",
    "    except ValueError:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at what phrases were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Doesn't Always Do What You Want\n",
    "\n",
    ">technique : multiplanar_td and td-weighted_images of the brain with gadolinium_according to standard departmental protocol ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_phrases = [w for w in trigram_generator[bigram_generator[blob.words]] if \"_\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
